# task-13-individual
The first chapter of Michael Nielsen's Neural Networks and Deep Learning introduces the basics of neural networks. It explains how they work, including perceptrons and feedforward neural networks, and how they can be trained using the backpropagation algorithm. Perceptrons are the simplest form of neural network, consisting of a single layer of neurons that apply linear transformations and activation functions to input data. Feedforward neural networks, on the other hand, include an input layer, one or more hidden layers, and an output layer, with each layer's neurons applying nonlinear activation functions like the sigmoid function.
To implement a neural network, several steps are involved. First, data preprocessing is necessary, which involves loading and preprocessing data, such as the MNIST dataset, using libraries like pickle and gzip. Next, a feedforward network is defined with specified layers and neurons, and weights and biases are initialized. Then, forward propagation is performed to compute the network's output. The backpropagation algorithm is used to calculate gradients and update parameters to improve the network's performance. Finally, the network's accuracy is assessed by comparing predictions to actual labels, gaining insights into how well the model generalizes to new data.
Through this process, practical skills are developed in using Python libraries like numpy and pickle, as well as development tools like VSCode for coding and debugging. Debugging is an essential part of the task, helping to address issues like incorrect data shapes or implementation errors. Overall, a comprehensive understanding of neural network fundamentals and practical experience in implementing and evaluating these models are acquired.

NUMPY VS TENSORFLOW
NumPy
Ease of Use: Difficult and error-prone, requires manual implementation of all neural network components (forward/backward propagation, gradient descent, etc.). Performance: Slow compared to deep learning libraries as it lacks GPU acceleration and optimized routines for matrix operations. Flexibility: Maximum flexibility; every aspect of the network can be customized, but each change must be implemented manually.

TensorFlow
Ease of Use: Very easy to use. TensorFlow abstracts most complexities, making it ideal for beginners or quick prototyping. Model definition and training are as simple as calling a few functions. Performance: Highly optimized, with support for GPU/TPU acceleration. Comparable to PyTorch in speed, and can be more efficient in distributed training scenarios. Flexibility: Less flexible than PyTorch. While it's customizable to an extent, the high-level Keras API abstracts away much of the detail, making it less suitable for non-standard architectures.paraphrase and make it a paragragh
